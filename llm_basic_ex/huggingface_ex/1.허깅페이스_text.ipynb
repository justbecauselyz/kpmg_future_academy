{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b465594-2d5b-4a79-90b4-09ade8d11ec8",
   "metadata": {},
   "source": [
    "# 허깅페이스 로그인 인증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1c6760-9a7f-4413-93de-78bc617a62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 방법1 : access token을 직접 입력하는 방법\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# # 방법2 : 환경변수에 저장, 로딩해서 인정하는 방법\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# # 1️⃣ .env 파일의 내용을 로드\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# # 2️⃣ 환경 변수 가져오기\n",
    "HF_READ_TOKEN = os.getenv(\"HF_READ_TOKEN\")\n",
    "# # print(HF_READ_TOKEN)\n",
    "\n",
    "# # 로그인 실행\n",
    "login(HF_READ_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04757e6f-7baa-4e1b-b800-d6222640e2b4",
   "metadata": {},
   "source": [
    "# 허깅페이스 transformers 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d251f67d-1a10-4d9f-9b49-0fc5ed5f2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae7286f-d7f6-400c-8b49-7687e5237c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a758c8e2-d528-4b0f-913e-e4d79da75b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '인공지능이란 무엇인가? 행로트는 도 포문하에이대 인고이 웄해서 아로하요 하는 합 어어요 가리 한 모르 다시 방자 아로하요 한 며자에 덐구라대 먴어요 한 별리 필어요 중배 하고에 봻을 맞어요 하고에 무에 모르 다시 방자 하는 장는 합하고에 아'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"인공지능이란 무엇인가?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73039499-c03b-4748-af42-7c63dcc1c4cd",
   "metadata": {},
   "source": [
    "=> text-generation 기본은 GPT-2를 다운 받음  \n",
    "=> GPT-2는 한국어를 잘 인식 못함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8fa45f5-2452-4a8a-9e0c-15359053666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is AI?\\n\\nAI is a new paradigm for information science that seeks to predict the future of human behavior. In this paper, we offer a computational framework for applying AI to a variety of behavioral problems, including self-reported anxiety, depression, and anxiety disorders. It is a framework for developing new, scalable, and efficient ways to predict the future by using artificial intelligence, machine learning, and machine learning approaches.\\n\\nThe Problem\\n\\nAI is the most promising form of information science, and it is a key aspect of our understanding of the future of human behavior.\\n\\nIn this paper, we present a set of five computational models for the problem of self-reported anxiety. We present them sequentially, and the model that we use is the second model. We refer to this model as the model of self-reported anxiety, because it is based on the idea that the brain produces a lot of self-reported anxiety, but that it does not produce self-reported anxiety at all. This model is called the self-reported anxiety model.\\n\\nThe Problem\\n\\nA person is an individual, and their behavior is based on the fact that they are feeling anxious. It is not the case that they are anxious in any way. Rather, they are feeling anxious because'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 인식하기\n",
    "result = pipe(\"What is AI?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311964e9-015a-4584-a9ff-06d10241372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      "\n",
      "AI is a new paradigm for information science that seeks to predict the future of human behavior. In this paper, we offer a computational framework for applying AI to a variety of behavioral problems, including self-reported anxiety, depression, and anxiety disorders. It is a framework for developing new, scalable, and efficient ways to predict the future by using artificial intelligence, machine learning, and machine learning approaches.\n",
      "\n",
      "The Problem\n",
      "\n",
      "AI is the most promising form of information science, and it is a key aspect of our understanding of the future of human behavior.\n",
      "\n",
      "In this paper, we present a set of five computational models for the problem of self-reported anxiety. We present them sequentially, and the model that we use is the second model. We refer to this model as the model of self-reported anxiety, because it is based on the idea that the brain produces a lot of self-reported anxiety, but that it does not produce self-reported anxiety at all. This model is called the self-reported anxiety model.\n",
      "\n",
      "The Problem\n",
      "\n",
      "A person is an individual, and their behavior is based on the fact that they are feeling anxious. It is not the case that they are anxious in any way. Rather, they are feeling anxious because\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffa38a-c545-4a70-b490-374890eea6e4",
   "metadata": {},
   "source": [
    "## 한국어 인식이 잘 되는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c44fdc-9033-49a1-9fcf-ea3361e9bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001FF6C4F7130>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "# task : text-generation\n",
    "# model : skt/kogpt2-base-v2\n",
    "\n",
    "text_gen_ko = pipeline(\"text-generation\", model=\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5b0623-8518-4cd9-a6d1-b2d14ddf99b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '인공지능이란? 그건 어떨까?\\n그런데 그걸 어떻게 할 수 있을까?\\n그런 궁금증을 해결해 줄 수 있는 장치가 있는지도 잘 모르겠어요.\\n이거 뭐야 뭘까? 하고 곰곰이 생각하다가 이게 왜 생겼나 하는 걸 알게 된 건데요.\\n그런데 문제는 이게 이제 한 번 더 그걸 해결해 주면 되겠다 하는 거죠.\\n그런데 그걸 좀 어마어마한 걸로 해결하고 싶다는 거죠.\\n진짜 그러니까 그걸 내가 어떻게 해 봐야 되냐? 그건 우리도 마찬가지예요.\\n그냥 어~ 이렇게 하면 되지 않냐? 네. 그거를 좀 설명해 주면 좋겠다 하는 거죠.\\n네. 이게 인제 이게 우리도 이제 알고 있고요.\\n그리고 이걸 좀 해 봐야 되는데 지금 이게 뭐 뭐냐면 이제 그까 우리가 예를 들어 뭐라고 하면 이게 어~ 그~ 뭐야 이게 뭐야 이거 이게 뭐야 이거 뭐야 뭘 이런 거야 이런 거 이런 거 아니겠습니까?\\n이런 거를 좀 만들어 봐야 되는데 그럴려면 이게 뭐야 이런 거 아니야 이게 뭐야 이게 뭐야 이런 거 아니야 이런 거 아니야 이런 거 아니야 이런 거 아니야 이런 거 아니야 이런 거 아니야'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_ko(\"인공지능이란?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2fd300-d96f-4c83-bca1-ea281a6f580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토끼는 왜인지 모를 정도.\n",
      "이런 묘사를 하기 위해 그가 만든 게 바로 그거라고 할 수 있을 정도.\n",
      "그가 개발한 건 마스다 코타로 ( 통칭 \"아이구 (! ) \".\n",
      "아테나와 비슷한 이름을 지닌 건 사실이지만 마스다 코타로라는 이름은 아리스에게 빌려쓰는 게 당연하다.\n",
      "마스다 코타로를 '모마모'라고 부르는 모양.\n",
      "마스다는 마스다 코타로의 여동생으로도 불리고 있다.\n",
      "여담으로 마스다가 본명을 밝힐때 벚꽃머리를 하고 있다.\n",
      "아마 마스다는 벚꽃과 연관되어 있어서인지 마스다가 아레나라고 호칭할 시 코타로와 시코가 같이 부른다고 한다. 2019년, 드디어 일본에서도 공식 SNS에서 팬심을 얻기 위한 다양한 이벤트가 진행된다.\n",
      "( 공식 웹페이지, 일본판 스토리와 일부 영상의 변경 ) \"신입생의 일본유튜브 인증 이벤트 \"유튜브 인증 이벤트\"를 진행 중이다. 작중 묘사를 보면 상당히 인상깊은 장면. 일본 유저들의 평가는 상당히 좋은 편이다.\n",
      "작중 초반부에 히키코모리 겐지랑 함께 등장해 히라타를 보고 히키코모리에게서 도망쳤나 보지만 히라타와 히노마루에게 저지당한다.\n",
      "그리고 마지막에 히라\n"
     ]
    }
   ],
   "source": [
    "# max_new_tokens: 256\n",
    "# do_sample: True\n",
    "# temperature: 0.7 (0.0은 허용하지 않음)\n",
    "result = text_gen_ko(\"토끼는\", do_sample = True, temperature=1.0, max_new_tokens=256)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d548ad0d-db76-4f91-9c5c-20a2bdbe4f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# LGAI-EXAONE/EXAONE-4.0-1.2B\n",
    "\n",
    "text_gen_ko = pipeline(\"text-generation\", model=\"LGAI-EXAONE/EXAONE-4.0-1.2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c194b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토끼는\n"
     ]
    }
   ],
   "source": [
    "result = text_gen_ko(\"토끼는\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2799e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|user|]\n",
      "너가 얼마나 대단한지 설명해 봐[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "저는 EXAONE으로, LG AI Research에서 개발된 대규모 언어 모델입니다. 제 능력은 다음과 같은 점에서 뛰어납니다:\n",
      "\n",
      "1. **복잡한 계산 처리**: 다양한 언어 작업을 빠르고 정확하게 수행할 수 있습니다.\n",
      "2. **다양한 언어 이해 및 생성**: 한국어, 영어 등 여러 언어를 유창하게 이해하고 생성할 수 있습니다.\n",
      "3. **빠른 응답 속도**: 긴 텍스트도 짧은 시간 내에 분석하고 요약하거나 새로운 답변을 제공할 수 있습니다.\n",
      "4. **학습 데이터 활용**: 방대한\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    # device_map=\"auto\" (GPU 사용시 활성화)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# choose your prompt\n",
    "prompt = \"Explain how wonderful you are\"\n",
    "prompt = \"Explica lo increíble que eres\"\n",
    "prompt = \"너가 얼마나 대단한지 설명해 봐\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56304840",
   "metadata": {},
   "source": [
    "# 감성 분석 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee413390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\")\n",
    "# classifier = pipeline(\"sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c9ec0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9977974891662598}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier(\"I love using Huggin Face transformers\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaef61ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998641014099121},\n",
       " {'label': 'NEGATIVE', 'score': 0.9954778552055359}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 문장을 한꺼번에 분류처리 할 때\n",
    "data = ['This restaurant serves delicious food.', \"I don't like to wake up early.\"]\n",
    "results = classifier(data)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac90da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2c0c2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This restaurant serves delicious food.', \"I don't like to wake up early.\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [미션] 아래와 같이 출력되도록 코드를 작성하세요.\n",
    "# This restaurant serves delicious food. - POSITIVE\n",
    "# I don't like to wake up early. - NEGATIVE\n",
    "\n",
    "data = [\"This restaurant serves delicious food.\", \"I don't like to wake up early.\"]\n",
    "# for sentencce in data:\n",
    "#     result = classifier(sentencce)\n",
    "#     label = result[0]['label']\n",
    "#     print(f\"{sentencce} - {label}\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa5423a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998641014099121},\n",
       " {'label': 'NEGATIVE', 'score': 0.9954778552055359}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4561c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This restaurant serves delicious food. - POSITIVE\n",
      "-\n",
      "I don't like to wake up early. - NEGATIVE\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "for sen, cl in zip(data, results):\n",
    "    print(f\"{sen} - {cl['label']}\")\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0e4e8",
   "metadata": {},
   "source": [
    "# 한국어 감성 분석\n",
    "- 텍스트에서 긍정 또는 부정으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd884a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6179131865501404},\n",
       " {'label': 'POSITIVE', 'score': 0.7464603781700134},\n",
       " {'label': 'POSITIVE', 'score': 0.842933714389801}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', ' 이 카페의 커피맛이 예술이네.']\n",
    "results = classifier(data)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d29e9",
   "metadata": {},
   "source": [
    "- 한국어를 잘못 분류했기 때문에 한국어 분류 가능한 모델을 사용 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a218be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0872d723116d4460be73c34d59706322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/895 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--WhitePeak--bert-base-cased-Korean-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab4c51f9a724df0a144648447851964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8818b7e14f4e6b9637e1fc9f607a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d580d9bcf4c94123a2c34a087df054d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x00000247B85C7BE0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e15956dda6c4f0488bff340ba03c512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.8098212480545044}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Labeel_0: negative\n",
    "# Labeel_1: positive\n",
    "\n",
    "\n",
    "sentiment_model = pipeline(model = \"WhitePeak/bert-base-cased-Korean-sentiment\")\n",
    "sentiment_model(\"매우 좋아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "661d5bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9972922205924988},\n",
       " {'label': 'LABEL_0', 'score': 0.9885053038597107},\n",
       " {'label': 'LABEL_1', 'score': 0.9662097692489624}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', ' 이 카페의 커피맛이 예술이네.']\n",
    "sentiment_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb7439fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "햄버거가 맛이 별로다 - negative\n",
      "나는 수영을 잘하지 못한다. - negative\n",
      "이 카페의 커피맛이 예술이네. - positive\n"
     ]
    }
   ],
   "source": [
    "# 문제\n",
    "# 햄버거가 맛이 별로다. - negative\n",
    "# 나는 수영을 잘하지 못한다. - negative\n",
    "# 이 카페의 커피맛이 예술이네.   - positive\n",
    "\n",
    "for sen, cl in zip(data, results):\n",
    "    if sentiment_model(sen)[0]['label'] == 'LABEL_0':\n",
    "        print(f\"{sen.strip()} - negative\")\n",
    "    else:\n",
    "        print(f\"{sen.strip()} - positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6179131865501404},\n",
       " {'label': 'POSITIVE', 'score': 0.7464603781700134},\n",
       " {'label': 'POSITIVE', 'score': 0.842933714389801}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bc37b8e",
   "metadata": {},
   "source": [
    "# 한글 인식이 잘 되는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "365dd30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705ae31e318544ff8896d759a0ca4fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--gemma-3-270m-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85981f92d0f048ddbd3e8d2443bf983d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc2fd570f9e4baf895657e3ef34907d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e89957042c34382aee83ce9bf2bbef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a642d8013704157a65585c91fb26549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a89b12a654345e5838521e507557c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b2ade8718a456781c4e4fb68744bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ecc1c9368f4ee78ae3c73f38cc6cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7062ff2054bd4fa889effbc72de4246f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = \"google/gemma-3-270m-it\"\n",
    "generation_pipeline = pipeline(\"text-generation\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa322fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능이란?\\n\\n인공지능은 인간의 능력을 컴퓨터가 통제하거나 학습하여 실현하는 기술입니다.\\n\\n인공지능은 다양한 분야에서 활용될 수 있으며, 특히 다음과 같은 분야에서 큰 영향을 미치고 있습니다.\\n\\n*   **의료:** 질병 진단, 치료, 환자 관리 등에 활용됩니다.\\n*   **재료 과학:** 새로운 소재 개발, 에너지 효율 향상, 환경 보호 등에 활용됩니다.\\n*   **금융:** 금융 상품 개발, 투자 전략 개선, 위험 관리 등에 활용됩니다.\\n*   **교육:** 학습 자료 제작, 교육 콘텐츠 개발, 학습 지원 등에 활용됩니다.\\n*   **생명공학:** 생명 윤리, 생물학적 연구 등에 활용됩니다.\\n\\n인공지능은 다양한 방식으로 변화하고 있으며, 앞으로도 더욱 발전하고 새로운 가능성을 제시할 것으로 기대됩니다.\\n\\n**인공지능의 주요 기술:**\\n\\n*   **딥러닝:** 머신러닝, 딥러닝의 핵심 기술로, 데이터 분석 및 예측을 위한 강력한 알고리즘입니다.\\n*   **로직 컴퓨팅:** 컴퓨터의 동작을 이해하고 복잡한 시스템을 제어하는 기술로, 자연어 처리'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline(\"인공지능이란?\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2c9aa",
   "metadata": {},
   "source": [
    "## 문장 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8837d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717039926c284758995c543328dbb604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9d9f27b60e4ee08d28b627acdbaf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e058e17d384b5c9aabc44526d7f58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x00000247B85C7BE0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce559ca7ded4eb29e2379e696fcf4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c584e3e5c5465a81790c5e428725e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summ = pipeline('summarization', framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc7fb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test= \"\"\"\n",
    "임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\n",
    "\n",
    "12일 아이즈(IZE) 취재 결과, 오는 22일 진행되는 KBS 2TV '불후의 명곡'(이하 '불후') 녹화가 '아티스트 이정현 편'으로 꾸며진다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c5ccd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0pronounced  \\xa0- \\xa0\\xa0-\\xa0- 'И� \\xa0' \\xa0KBS 2TV '불후의   테크   '텬 \\xa0’  ’�’ – \\xa0 '명’ - is a Korean TV show hosted by KBS 2 TV .\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ(sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf71ba6",
   "metadata": {},
   "source": [
    "- 한글 인식이 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "040122ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b8d7cdd2be4e5f825eb8a43ebf968f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--psyche--KoT5-summarization. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a3ca0579af484f85bdfc84d3a1f185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e810ec4369d4223bb2a5b903a2bc2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b834f0f5350e4cfd8b47e64727f5d86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2524fb4b7a4d2082c92f8812fdc195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# psyche/KoT5-summarization\n",
    "summ = pipeline('summarization', model=\"psyche/KoT5-summarization\", framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "844dd9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test= \"\"\"\n",
    "임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\n",
    "\n",
    "12일 아이즈(IZE) 취재 결과, 오는 22일 진행되는 KBS 2TV '불후의 명곡'(이하 '불후') 녹화가 '아티스트 이정현 편'으로 꾸며진다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37dada1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\"}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ(sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69489e",
   "metadata": {},
   "source": [
    "### 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32b90e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74020cf3005547b490953e405faa7493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9fe05917b476d8959ad6249ba6054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0567efee14f8458aa6d87dd58f8105ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cc73fbee5e49a5a3fed32b78016d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36ec9046f094ca99556fd1a11377916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6ef46a95294102913ba1fe0188be94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbc5e3bb0a341de991b1f6feba1993d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Gee. I'm the youngest in my class. I'm going to go to school. I'm going to be a scientist. I'm going to have a job. I'm going to do something that will make a difference in your life. I'm going to help you. I'm going to get you out of poverty.\"\n",
      "\n",
      "She continued, \"I have a job. I have a family. I have friends. I have a home. I have a job, and I'm going to do something that will make a difference in your life.\"\n",
      "\n",
      "\"I'm going to help you,\" his words echoed through the room.\n",
      "\n",
      "And then he got off his bike and went to his apartment. He asked, \"I want you to take a look at my bedroom. I want you to look at my room. I want you to look at your room. I want you to look at my bedroom. I want you to go to your room and say, 'What the hell is going on?' \"\n",
      "\n",
      "\"What is going on?\" he asked.\n",
      "\n",
      "\"I'm going to leave it to you to make sure you are not alone,\" he added.\n",
      "\n",
      "So, he left.\n",
      "\n",
      "And then he took a photo of\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe= pipeline(\"text-generation\", model=\"gpt2\", framework=\"pt\")\n",
    "print(pipe(\"Hello, my name is\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f498b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
