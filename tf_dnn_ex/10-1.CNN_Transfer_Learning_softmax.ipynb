{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8323afb8-da43-4abd-8075-bd9821ffb596",
   "metadata": {},
   "source": [
    "# Transfer Learning (전이학습)\n",
    "- 똑똑한 사람이 만들어 놓은 것을 가져가서 쓰는 것\n",
    "- imagenet 데이터를 학습한 것을 사용\n",
    "- mscoco image 데이터도 있음\n",
    "- 사전학습된 모델: base (CNN부분) + haed(ANN부분)\n",
    "- base(특징 추출을 잘함)는 그대로 쓰고, head 부분을 재구성\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b6563-0a88-4cc6-85bb-606d14fe7424",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3eb9117-b218-4971-988f-d2cd2a523ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile, pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c5c8e-ef0c-4724-9d31-e4e9312b0f83",
   "metadata": {},
   "source": [
    "# Step 1. Downloading the Dogs vs Cats dataset\n",
    "## 데이터 준비\n",
    "- 데이터가 있다면 실행 안해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a417c7e2-22fe-4c67-9501-a1f930a7dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축을 풀 위치\n",
    "extract_root = pathlib.Path(\"./datas_dnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eae822-a75a-4fb2-bafb-46a37c7b951d",
   "metadata": {},
   "source": [
    "### import project dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f29807-904d-4c1a-9046-a259cd5fa8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0f395-ff26-4811-aa02-d63edbaf1968",
   "metadata": {},
   "source": [
    "# Step 2. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cf0536-5a48-4b93-af3b-2ad81d89e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = extract_root / 'cats_and_dogs_filtered'\n",
    "train_dir  = BASE_DIR / 'train'\n",
    "val_dir = BASE_DIR / 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22c0214-75d9-417a-9164-dba54638b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128  # 모바일넷 모델 이미지 기준\n",
    "IMAGE_SHAPE= (IMG_SIZE, IMG_SIZE, 3)\n",
    "BATCH_SIZE = 128\n",
    "SEED = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba40c3cd-9248-4dd1-93b5-6d2f54548b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2001 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size = (IMG_SIZE, IMG_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed = SEED\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae93ce6c-c702-4c50-b4f5-db2320ca7078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1003 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size = (IMG_SIZE, IMG_SIZE) ,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397246ef-4999-40be-b5b7-7df7aaf206a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e6d4ab-9cb5-4e4c-a339-2c495e4baa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b8ffea-4244-4919-a94d-93c6585febb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62aca611-9708-4cef-80b4-0a91638740a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = val_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7212dd9d-5036-487d-84e9-92c7506e5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomTranslation(0.1, 0.1)\n",
    "    ],\n",
    "    name = 'data_augmentation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4f8eb-8e04-412a-a4ce-dd86f6e741e0",
   "metadata": {},
   "source": [
    "# Step 3. Building the model: MobileNetV2를 활용\n",
    "## Loading the pre-trained model (MovileNetV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270e1bc-ff0a-42e3-8764-0c4403f41824",
   "metadata": {},
   "source": [
    "### 1) 사전학습 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f655fa6b-3611-4605-8311-41d59c7e687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습된 모델: base(CNN 부분) + head(ANN 부분)\n",
    "# include=False: 분류기(FC Layer) 제거, CNN의 feature extractor 부분만 사용\n",
    "base_model=MobileNetV2(input_shape=IMAGE_SHAPE,include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd136f8-7a79-4f9e-bb77-95ce948e0bd2",
   "metadata": {},
   "source": [
    "### 2) Freezing the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67837316-cba5-44b1-9ce4-d389c5003f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습된 가중치를 그대로 사용하고, 학습 동안 업데이트하지 않겠다\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8803f5-ee39-4524-b261-17156d0e3f7e",
   "metadata": {},
   "source": [
    "### 3) 모델의 head부분 새롭게 정의\n",
    "Defining the model\n",
    "- functional API라는 방식으로 트랜스퍼 러닝을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72df48f-95ff-4174-8211-29d5345a709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 입력층 정의\n",
    "inputs = keras.Input(shape=IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f8ba714-bf05-4765-8e3b-2b7f54d7ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강\n",
    "x=data_augmentation(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852cf7c9-9597-41c9-8817-4ce98cdf696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling (모바일넷 학습할 떄 이렇게 했다고 함 -> 메뉴얼에 나와있는내용. 이렇게 했을 때 학습이 잘된다고 함)\n",
    "x=layers.Rescaling(scale=1.0/127.5, offset=-1.0)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f59b26c-65aa-49e4-8cb6-3ee13187ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델에 전처리한 데이터 넣기\n",
    "# training=False: 해당 모델을 학습 모드가 아니라,예측(inference) 모드로 실행\n",
    "x=base_model(x, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d17c75ab-7b3b-4a78-9c20-616225200dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균은 flattern과 동일 (각 행렬의 대푯값 하나를 추출)\n",
    "# FC층에전달하기 전 가장 많이 쓰는 \"특징 요약\" 방식\n",
    "# MobileNetV2 전이학습 구조\n",
    "# CNN -> GlobalAveragePooling2D -> FC\n",
    "# Flattern을 사용해도 됨\n",
    "x=layers.GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f3e1c32-6bd4-4622-b98e-ff0d60ec4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connection층\n",
    "x=layers.Dense(128, keras.activations.relu)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56d86ec6-df24-49ff-ba1d-254a084a8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층\n",
    "# outputs=layers.Dense(1, keras.activations.sigmoid)(x)\n",
    "outputs=layers.Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27c25f47-11c9-4c6b-9c58-977cb283d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기\n",
    "model=keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27460cfe-257c-4454-a435-5b606767e64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_128 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m163,968\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,210</span> (9.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,210\u001b[0m (9.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,226</span> (641.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,226\u001b[0m (641.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base_model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c7096-39f3-4027-a9eb-6db77a47b0f2",
   "metadata": {},
   "source": [
    "### 4) Compiling the model\n",
    "- 현업에서 일반적으로 전이학습 시 러닝메이트를 0.0001~0.001을 많이 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bee11d7-662d-4b02-bd00-c561d11b8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층 sigmoid인 경우\n",
    "# model.compile(optimizer=keras.optimizers.RMSprop(\n",
    "#             learning_rate=1e-4),\n",
    "#             loss=keras.losses.BinaryCrossentropy(),\n",
    "#             metrics = [ keras.metrics.BinaryAccuracy ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5da93fd3-aeb3-46c8-a548-06e71d780b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax일 경우 꼭 해주기\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed8018eb-b876-4ed5-a5c4-9e5dc23f3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bd224-80de-40bc-8f51-70a0c6fa445a",
   "metadata": {},
   "source": [
    "### 5) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a4cd8ae-d6b8-4a1b-9b9e-acbafc3273f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - loss: 0.6903 - sparse_categorical_accuracy: 0.6517 - val_loss: 0.3622 - val_sparse_categorical_accuracy: 0.8544\n",
      "Epoch 2/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 959ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.8801 - val_loss: 0.1975 - val_sparse_categorical_accuracy: 0.9302\n",
      "Epoch 3/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 941ms/step - loss: 0.2376 - sparse_categorical_accuracy: 0.9005 - val_loss: 0.1436 - val_sparse_categorical_accuracy: 0.9501\n",
      "Epoch 4/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 924ms/step - loss: 0.2022 - sparse_categorical_accuracy: 0.9240 - val_loss: 0.1194 - val_sparse_categorical_accuracy: 0.9571\n",
      "Epoch 5/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 940ms/step - loss: 0.1666 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.1068 - val_sparse_categorical_accuracy: 0.9641\n",
      "Epoch 6/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 933ms/step - loss: 0.1618 - sparse_categorical_accuracy: 0.9375 - val_loss: 0.0962 - val_sparse_categorical_accuracy: 0.9631\n",
      "Epoch 7/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 943ms/step - loss: 0.1554 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.0906 - val_sparse_categorical_accuracy: 0.9691\n",
      "Epoch 8/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 938ms/step - loss: 0.1496 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.0831 - val_sparse_categorical_accuracy: 0.9701\n",
      "Epoch 9/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 932ms/step - loss: 0.1403 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.0801 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 10/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 941ms/step - loss: 0.1359 - sparse_categorical_accuracy: 0.9455 - val_loss: 0.0746 - val_sparse_categorical_accuracy: 0.9761\n",
      "Epoch 11/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 929ms/step - loss: 0.1319 - sparse_categorical_accuracy: 0.9465 - val_loss: 0.0747 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 12/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 947ms/step - loss: 0.1112 - sparse_categorical_accuracy: 0.9555 - val_loss: 0.0690 - val_sparse_categorical_accuracy: 0.9771\n",
      "Epoch 13/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 929ms/step - loss: 0.1314 - sparse_categorical_accuracy: 0.9450 - val_loss: 0.0737 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 14/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 927ms/step - loss: 0.1209 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.0668 - val_sparse_categorical_accuracy: 0.9761\n",
      "Epoch 15/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 933ms/step - loss: 0.1205 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.0707 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 16/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 941ms/step - loss: 0.1129 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.0648 - val_sparse_categorical_accuracy: 0.9791\n",
      "Epoch 17/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 940ms/step - loss: 0.1032 - sparse_categorical_accuracy: 0.9565 - val_loss: 0.0656 - val_sparse_categorical_accuracy: 0.9771\n",
      "Epoch 18/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 946ms/step - loss: 0.1127 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.0688 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 19/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 931ms/step - loss: 0.1018 - sparse_categorical_accuracy: 0.9590 - val_loss: 0.0613 - val_sparse_categorical_accuracy: 0.9791\n",
      "Epoch 20/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 928ms/step - loss: 0.0972 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.0652 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 21/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 945ms/step - loss: 0.0981 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.0638 - val_sparse_categorical_accuracy: 0.9751\n",
      "Epoch 22/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 937ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.9570 - val_loss: 0.0580 - val_sparse_categorical_accuracy: 0.9801\n",
      "Epoch 23/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 932ms/step - loss: 0.0979 - sparse_categorical_accuracy: 0.9610 - val_loss: 0.0649 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 24/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 945ms/step - loss: 0.0969 - sparse_categorical_accuracy: 0.9645 - val_loss: 0.0546 - val_sparse_categorical_accuracy: 0.9821\n",
      "Epoch 25/25\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 928ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9695 - val_loss: 0.0627 - val_sparse_categorical_accuracy: 0.9761\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_ds, epochs=25, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4740a-e887-4e18-a0a2-0ec903c44fa8",
   "metadata": {},
   "source": [
    "### 6) Transfer learning model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d779da-ee0c-4696-b7f0-aa23b66a4fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 509ms/step - loss: 0.0627 - sparse_categorical_accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06266587972640991, 0.9760717749595642]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8/8 ━━━━━━━━━━━━━━━━━━━━ 4s 501ms/step - loss: 0.0600 - sparse_categorical_accuracy: 0.9771\n",
    "# [0.059976693242788315, 0.9770687818527222]\n",
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff705368-8062-46cd-b7d8-19ba0643c7a9",
   "metadata": {},
   "source": [
    "## 모델 일반화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "777cfd04-0cfc-4d21-8aa8-e10240972f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00347302-077d-47c3-a9d4-d18edd971445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 파일 선택 다이얼로그 열기\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # GUI 창 숨기기\n",
    "file_paths = 'datas_dnn/Sunflower_sky_backdrop.jpg'\n",
    "\n",
    "# 2) 선택한 파일들 예측\n",
    "for fname in file_paths:\n",
    "    img = load_img(fname, target_size=(IMG_SIZE, IMG_SIZE), color_mode=\"rgb\")\n",
    "    x = img_to_array(img)            # (H,W,C)\n",
    "    x = np.expand_dims(x, axis=0)    # (1,H,W,C)\n",
    "    x = preprocess_input(x)          # MobileNetV2: [-1,1] 스케일\n",
    "\n",
    "    probs = model.predict(x, verbose=0)[0]     # shape = (2,)\n",
    "    pred_index = np.argmax(probs)\n",
    "    pred_class = class_names[pred_index]\n",
    "    pred_prob = probs[pred_index]\n",
    "\n",
    "    fname = fname.split(\"/\")[-1]\n",
    "    print(f\"{fname} -> class={pred_class}, prob={pred_prob:.4f}, full_probs={probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edb22b-01c5-46f6-80f1-caab009c0d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
